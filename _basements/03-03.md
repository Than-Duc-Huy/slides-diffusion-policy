---
slide: 03
---

### Development of approaches

- Explicit Policy: The network learns a direct mapping
    - Given observation, output action
    - Performs poorly when there is discontinuity (from [IBC Paper](https://arxiv.org/pdf/2109.00137))

- Implicit Policy: The network learns an energy manifold/landscape
    - Given observation and action, output this energy manifold. Find minimal point to find the action

- Diffusion Policy: The network learns the gradient over time steps to turn noisy actions into desired actions
    - Start from a noisy action, conditioned on the observation, output the direction that leads to the desired action

<img src="assets/img/dev.png" height="300" width="auto" />
